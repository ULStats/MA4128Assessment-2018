**Deep Learning**

Mikie Lyons

![](http://www.schoolmeet.org/wordpress/wp-content/uploads/2016/07/DeepLearning.jpg)

Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods
based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or
unsupervised.
Deep learning models are loosely related to information processing and communication patterns in a biological nervous system, such as
neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain.
Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields
including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine
translation, bioinformatics and drug design, where they have produced results comparable to and in some cases superior to human experts.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/512px-Kernel_Machine.svg.png)

* History
The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986, and to Artificial Neural Networks by
Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey
Ivakhnenko and Lapa in 1965. A 1971 paper described a deep network with 8 layers trained by the group method of data handling algorithm.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition
(ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of
large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks (CNNs) were superseded for ASR by CTC 
for LSTM, but are more successful in computer vision.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks
written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around
2010.
In 2012, a team led by Dahl won the "Merck Molecular Activity Challenge" using multi-task deep neural networks to predict the
biomolecular target of one drug. In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of
environmental chemicals in nutrients, household products and drugs and won the "Tox21 Data Challenge" of NIH, FDA and NCATS.

![](http://andrewyuan.github.io/img/DeepLearning.png)
![](https://blogs.nvidia.com/wp-content/uploads/2016/07/Deep_Learning_Icons_R5_PNG.jpg.png)

Deep learning is a class of machine learning algorithms that:
* Use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation. Each successive layer uses
the output from the previous layer as input.
* Learn in supervised (e.g., classification) and/or unsupervised (e.g., pattern analysis) manners.
* Learn multiple levels of representations that correspond to different levels of abstraction; the levels form a hierarchy of concepts.

In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own.

The "deep" in "deep learning" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized).


## References

